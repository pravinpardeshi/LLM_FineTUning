{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4sJFkQUxZwsHZ18Rl+DL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pravinpardeshi/LLM_FineTUning/blob/main/FT_Working_30May2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr7nwBAyJLmQ"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "2aDa47FhKGiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/Llama-3.2-1B\",\n",
        "        load_in_4bit = True,\n",
        "        max_seq_length = 1024,\n",
        "        dtype = None\n",
        "    )"
      ],
      "metadata": {
        "id": "-n304dlzKQxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "NJipY1H9zvIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = 16, # Suggested 8, 16, 32, 64, 128\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "        lora_alpha = 16,\n",
        "        lora_dropout = 0,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = True,\n",
        "        random_state = 3407,\n",
        "        use_rslora = False, # rank stabilized LoRA\n",
        "        loftq_config = None\n",
        "    )"
      ],
      "metadata": {
        "id": "Tmn4Q9YEKVQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LkuN6fxAsX8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import json, yaml\n",
        "import torch"
      ],
      "metadata": {
        "id": "YS1pwraaLjSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "data_files = {\n",
        "    \"train\": \"20_records.json\",\n",
        "    \"validation\": \"eval.json\"\n",
        "}\n",
        "\n",
        "print(data_files['validation'])\n",
        "\n",
        "# Load JSON files using pandas\n",
        "train_df = pd.read_json(data_files[\"train\"])\n",
        "validation_df = pd.read_json(data_files[\"validation\"])\n",
        "\n",
        "# Convert pandas DataFrames to Dataset objects\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "validation_dataset = Dataset.from_pandas(validation_df)\n",
        "\n",
        "# Combine the datasets into a DatasetDict\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n",
        "\n",
        "#dataset = load_dataset(\"json\", data_files=data_files)"
      ],
      "metadata": {
        "id": "jvKUI_Dcr0f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = prompt_format.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Uploaded custom data json into colab's session\n",
        "#with open('37_records.csv', 'r') as f:\n",
        "\n",
        "#    json_f = yaml.safe_load(f.read())\n",
        "\n",
        "#df = pd.DataFrame(json_f)\n",
        "#print(df.columns)\n",
        "\n",
        "#dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n"
      ],
      "metadata": {
        "id": "EG29LsD3KYFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.column_names"
      ],
      "metadata": {
        "id": "i3cXF8jPLt0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import  is_bfloat16_supported\n"
      ],
      "metadata": {
        "id": "QyOqr0kDL7ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "train_args = SFTConfig(\n",
        "    auto_find_batch_size = True,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_ratio = 0.1,\n",
        "    num_train_epochs = 5,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    optim = \"adamw_torch\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    logging_strategy = \"epoch\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    metric_for_best_model = \"eval_loss\",\n",
        "    load_best_model_at_end = True,\n",
        "    save_strategy = \"epoch\",\n",
        "    save_total_limit = 1,\n",
        "    output_dir = \"outputs\"\n",
        ")"
      ],
      "metadata": {
        "id": "oToMitZtLw2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    eval_dataset = dataset[\"validation\"],\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 1024,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = train_args\n",
        ")"
      ],
      "metadata": {
        "id": "T80EzAJ9MD7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_status = trainer.train()"
      ],
      "metadata": {
        "id": "y96R8C9fx6QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt_format.format(\n",
        "        \"You are an expert at drawing inferences based on your knowledge.\", # instruction\n",
        "        \"Based on your knowledge, create the best answer for the question asked on the Go Getter Book. If you do not know then say, you cannot answer the question.\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "XQdhxtbOXXtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = True)\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "print(response[0].split(\"### Response:\")[1].strip())"
      ],
      "metadata": {
        "id": "3BwtKzTYXaRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8\n",
        "# Save locally as sharded model files\n",
        "# model.save_pretrained_merged(\"amrs_csv_gen_model\", tokenizer, save_method = \"merged_16bit\", )"
      ],
      "metadata": {
        "id": "CHwi4ar3Xfg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 9\n",
        "# Push to Huggingface hub - replace the space username as required\n",
        "#model.push_to_hub_merged(\"amrs-tech/csv_gen_model\",\n",
        "#\t\t\t\t\t\t\ttokenizer, save_method = \"merged_16bit\",\n",
        "#\t\t\t\t\t\t\ttoken = \"hf_HqxQEPdJ******enubpWyh\"\n",
        "#\t\t\t\t\t\t\t)"
      ],
      "metadata": {
        "id": "_jog6jB1Xjr_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}